{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv('../data/processed/train_data_processed.csv')\n",
    "test = pd.read_csv('../data/processed/test_data_processed.csv')\n",
    "val = pd.read_csv('../data/processed/val_data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more feature engineering\n",
    "# use encoder to encode OCCURRED_ON_DATE column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train['OCCURRED_ON_DATE'] = le.fit_transform(train['OCCURRED_ON_DATE'])\n",
    "test['OCCURRED_ON_DATE'] = le.transform(test['OCCURRED_ON_DATE'])\n",
    "val['OCCURRED_ON_DATE'] = le.transform(val['OCCURRED_ON_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/datetime_encoder.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save le \n",
    "import joblib\n",
    "joblib.dump(le, '../models/datetime_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop _id column\n",
    "\n",
    "test = test.drop('_id', axis=1)\n",
    "val = val.drop('_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the target variable\n",
    "y_train = train['Severe_crimes']\n",
    "y_test = test['Severe_crimes']\n",
    "y_val = val['Severe_crimes']\n",
    "\n",
    "# define the features\n",
    "X_train = train.drop(['Severe_crimes'], axis=1)\n",
    "X_test = test.drop(['Severe_crimes'], axis=1)\n",
    "X_val = val.drop(['Severe_crimes'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9943762120232709\n",
      "Precision:  0.9923076923076923\n",
      "Recall:  0.9186164801627671\n",
      "F1:  0.954041204437401\n"
     ]
    }
   ],
   "source": [
    "# build a random forest model which we selected as the best model from last week\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# the random forest model will have 1000 trees and a max depth of 10\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=42)\n",
    "# fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# make predictions\n",
    "y_pred = rf.predict(X_val)\n",
    "# calculate the evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "# print the evaluation metrics\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1: ', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bootstrapping to the model\n",
    "from sklearn.utils import resample\n",
    "# define the number of bootstraps\n",
    "n_bootstraps = 100\n",
    "# create empty lists to store the evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "# loop through the number of bootstraps\n",
    "for i in range(n_bootstraps):\n",
    "    # resample the data\n",
    "    X_resampled, y_resampled = resample(X_train, y_train, random_state=i)\n",
    "    # fit the model\n",
    "    rf.fit(X_resampled, y_resampled)\n",
    "    # make predictions\n",
    "    y_pred = rf.predict(X_val)\n",
    "    # calculate the evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    # append the evaluation metrics to the lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# calculate the mean and standard deviation of the evaluation metrics\n",
    "accuracy_mean = np.mean(accuracy_scores)\n",
    "accuracy_std = np.std(accuracy_scores)\n",
    "precision_mean = np.mean(precision_scores)\n",
    "\n",
    "precision_std = np.std(precision_scores)\n",
    "recall_mean = np.mean(recall_scores)\n",
    "recall_std = np.std(recall_scores)\n",
    "f1_mean = np.mean(f1_scores)\n",
    "f1_std = np.std(f1_scores)\n",
    "\n",
    "# print the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9942533936651584 +/- 0.0001544641647750288\n",
      "Precision:  0.9932187827422044 +/- 0.0014498888540798896\n",
      "Recall:  0.9158189216683621 +/- 0.002673664618708329\n",
      "F1:  0.9529464309013826 +/- 0.001320692043123352\n"
     ]
    }
   ],
   "source": [
    "# print the evaluation metrics\n",
    "print('Accuracy: ', accuracy_mean, '+/-', accuracy_std)\n",
    "print('Precision: ', precision_mean, '+/-', precision_std)\n",
    "print('Recall: ', recall_mean, '+/-', recall_std)\n",
    "print('F1: ', f1_mean, '+/-', f1_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFFENSE_DESCRIPTION: 0.7410702456668807\n",
      "OFFENSE_CODE: 0.2352010942417921\n",
      "HOUR: 0.008336455421575177\n",
      "OCCURRED_ON_DATE: 0.005759008230516233\n",
      "DISTRICT: 0.004799620122936456\n",
      "DAY_OF_WEEK: 0.002475637220088287\n",
      "MONTH: 0.0023579390962110186\n"
     ]
    }
   ],
   "source": [
    "# get the more important features list\n",
    "importances = rf.feature_importances_\n",
    "# sort the importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# get the feature names\n",
    "features = X_train.columns\n",
    "\n",
    "# show the list of features and their importances\n",
    "for i in range(X_train.shape[1]):\n",
    "    print(f'{features[indices[i]]}: {importances[indices[i]]}')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCAIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
